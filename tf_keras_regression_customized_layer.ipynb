{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_keras_regression-customized_layer",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHwb3C0MLINMxJQvsQy8L0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmeraldMath/Tensorflow_models/blob/master/tf_keras_regression_customized_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qTaQkiL_QP3",
        "colab_type": "code",
        "outputId": "666bd9b4-ed5a-4e75-8dc4-c3aa885f6447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "matplotlib 3.1.3\n",
            "numpy 1.17.5\n",
            "pandas 0.25.3\n",
            "sklearn 0.22.1\n",
            "tensorflow 2.1.0\n",
            "tensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUpI_AwBhwHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b7fd8ff-e1c0-4ea8-f8d7-b92510f852f6"
      },
      "source": [
        "layer = tf.keras.layers.Dense(100)\n",
        "#None means notdetermined number of sample\n",
        "layer = tf.keras.layers.Dense(100, input_shape=(None, 5))\n",
        "layer(tf.zeros([10,5]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 100), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvbD2YSBs46V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55b4df06-daa9-4e19-fce1-c2fd8510b0ab"
      },
      "source": [
        "#layer.variables\n",
        "layer.trainable_variables\n",
        "help(layer)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
            "\n",
            "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
            " |  Just your regular densely-connected NN layer.\n",
            " |  \n",
            " |  `Dense` implements the operation:\n",
            " |  `output = activation(dot(input, kernel) + bias)`\n",
            " |  where `activation` is the element-wise activation function\n",
            " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
            " |  created by the layer, and `bias` is a bias vector created by the layer\n",
            " |  (only applicable if `use_bias` is `True`).\n",
            " |  \n",
            " |  Note: If the input to the layer has a rank greater than 2, then\n",
            " |  it is flattened prior to the initial dot product with `kernel`.\n",
            " |  \n",
            " |  Example:\n",
            " |  \n",
            " |  ```python\n",
            " |  # as first layer in a sequential model:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, input_shape=(16,)))\n",
            " |  # now the model will take as input arrays of shape (*, 16)\n",
            " |  # and output arrays of shape (*, 32)\n",
            " |  \n",
            " |  # after the first layer, you don't need to specify\n",
            " |  # the size of the input anymore:\n",
            " |  model.add(Dense(32))\n",
            " |  ```\n",
            " |  \n",
            " |  Arguments:\n",
            " |    units: Positive integer, dimensionality of the output space.\n",
            " |    activation: Activation function to use.\n",
            " |      If you don't specify anything, no activation is applied\n",
            " |      (ie. \"linear\" activation: `a(x) = x`).\n",
            " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
            " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
            " |    bias_initializer: Initializer for the bias vector.\n",
            " |    kernel_regularizer: Regularizer function applied to\n",
            " |      the `kernel` weights matrix.\n",
            " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
            " |    activity_regularizer: Regularizer function applied to\n",
            " |      the output of the layer (its \"activation\")..\n",
            " |    kernel_constraint: Constraint function applied to\n",
            " |      the `kernel` weights matrix.\n",
            " |    bias_constraint: Constraint function applied to the bias vector.\n",
            " |  \n",
            " |  Input shape:\n",
            " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
            " |    The most common situation would be\n",
            " |    a 2D input with shape `(batch_size, input_dim)`.\n",
            " |  \n",
            " |  Output shape:\n",
            " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
            " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
            " |    the output would have shape `(batch_size, units)`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Dense\n",
            " |      tensorflow.python.keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Creates the variables of the layer (optional, for subclass implementers).\n",
            " |      \n",
            " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
            " |      can override if they need a state-creation step in-between\n",
            " |      layer instantiation and layer call.\n",
            " |      \n",
            " |      This is typically used to create the weights of `Layer` subclasses.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
            " |          `TensorShape` if the layer expects a list of inputs\n",
            " |          (one instance per input).\n",
            " |  \n",
            " |  call(self, inputs)\n",
            " |      This is where the layer's logic lives.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Input tensor, or list/tuple of input tensors.\n",
            " |          **kwargs: Additional keyword arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor or list/tuple of tensors.\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, inputs, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, inputs=None)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(inputs, self):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Actvity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
            " |      specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
            " |          passed, it signals the losses are conditional on some of the layer's\n",
            " |          inputs, and thus they should only be run where these inputs are\n",
            " |          available. This is the case for activity regularization losses, for\n",
            " |          instance. If `None` is passed, the losses are assumed\n",
            " |          to be unconditional, and will apply across all dataflows of the layer\n",
            " |          (e.g. weight regularization losses).\n",
            " |  \n",
            " |  add_metric(self, value, aggregation=None, name=None)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
            " |          it indicates that the metric tensor provided has been aggregated\n",
            " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
            " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
            " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
            " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
            " |          aggregation='mean')`.\n",
            " |        name: String metric name.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
            " |      \n",
            " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      `inputs` is now automatically inferred\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
            " |      specific set of inputs.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Arguments:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
            " |      \n",
            " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      Please use `layer.add_weight` method instead.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
            " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
            " |        instance is returned.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called with partitioned variable regularization and\n",
            " |          eager execution is enabled.\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! (deprecated)\n",
            " |      \n",
            " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      Please use `layer.__call__` method instead.\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Returns the current weights of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Weights values as a list of numpy arrays.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  from_config(config) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  dtype\n",
            " |  \n",
            " |  dynamic\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |  \n",
            " |  losses\n",
            " |      Losses which are associated with this `Layer`.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  metrics\n",
            " |  \n",
            " |  name\n",
            " |      Returns the name of this module as passed or determined in the ctor.\n",
            " |      \n",
            " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
            " |      parent module names.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  trainable_weights\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      ```\n",
            " |      class MyModule(tf.Module):\n",
            " |        @tf.Module.with_name_scope\n",
            " |        def __call__(self, x):\n",
            " |          if not hasattr(self, 'w'):\n",
            " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
            " |          return tf.matmul(x, self.w)\n",
            " |      ```\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      ```\n",
            " |      mod = MyModule()\n",
            " |      mod(tf.ones([8, 32]))\n",
            " |      # ==> <tf.Tensor: ...>\n",
            " |      mod.w\n",
            " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      ```\n",
            " |      a = tf.Module()\n",
            " |      b = tf.Module()\n",
            " |      c = tf.Module()\n",
            " |      a.b = b\n",
            " |      b.c = c\n",
            " |      assert list(a.submodules) == [b, c]\n",
            " |      assert list(b.submodules) == [c]\n",
            " |      assert list(c.submodules) == []\n",
            " |      ```\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saB-LVmq_T6P",
        "colab_type": "code",
        "outputId": "6eec703c-0f6d-409e-f60f-24a83d1f14cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "print(housing.DESCR)\n",
        "print(housing.data.shape)\n",
        "print(housing.target.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block\n",
            "        - HouseAge      median house age in block\n",
            "        - AveRooms      average number of rooms\n",
            "        - AveBedrms     average number of bedrooms\n",
            "        - Population    block population\n",
            "        - AveOccup      average house occupancy\n",
            "        - Latitude      house block latitude\n",
            "        - Longitude     house block longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "http://lib.stat.cmu.edu/datasets/\n",
            "\n",
            "The target variable is the median house value for California districts.\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n",
            "(20640, 8)\n",
            "(20640,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYWEvKM4JrZZ",
        "colab_type": "code",
        "outputId": "c3123a0c-9c79-4463-e35a-ef4ed1a81c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state = 7) #default test_size = 0.25\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train_all, y_train_all, random_state = 11)\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_valid.shape, y_valid.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11610, 8) (11610,)\n",
            "(3870, 8) (3870,)\n",
            "(5160, 8) (5160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iC77xYJKYs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_valid_scaled = scaler.transform(x_valid)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYKxGe3nF8My",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec5a2706-ae4a-4fc3-f077-b77f91b2c8c3"
      },
      "source": [
        "# tf.nn.softplus: log(1+e^x)\n",
        "customized_softplus = keras.layers.Lambda(lambda x: tf.nn.softplus(x))\n",
        "print(customized_softplus([-10., -5., 0., 5., 10.]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9uaKZuuKvtf",
        "colab_type": "code",
        "outputId": "35aef70e-f09f-40ec-876c-65a47c8064f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# customized dense layer\n",
        "class CustomizedDenseLayer(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "      self.units = units\n",
        "      self.activation = keras.layers.Activation(activation)\n",
        "      super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      \"\"\"build the needed parameters kernel w and bias b\"\"\"\n",
        "      # x * w + b x:input_shape: [None, a] output_shape [None, b]\n",
        "      # w: [a, b]\n",
        "      self.kernel = self.add_weight(name = 'kernel',\n",
        "                                    shape = (input_shape[1], self.units),\n",
        "                                    initializer = 'uniform',\n",
        "                                    trainable = True)\n",
        "      self.bias = self.add_weight(name = 'bias',\n",
        "                                 shape = (self.units, ),\n",
        "                                 initializer = 'zeros',\n",
        "                                 trainable = True)\n",
        "      super(CustomizedDenseLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "      \"\"\"forward\"\"\"\n",
        "      return  self.activation(x @ self.kernel + self.bias)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    CustomizedDenseLayer(30, activation='relu',\n",
        "                       input_shape = x_train.shape[1:]),\n",
        "    CustomizedDenseLayer(1),\n",
        "    customized_softplus,\n",
        "    # above equals to\n",
        "    # keras.layers.Dense(1, activation='softplus')\n",
        "    # or it also equals to\n",
        "    # keras.layers.Dense(1), keras.layers.Activation('softplus'),                  \n",
        "])\n",
        "model.summary()\n",
        "model.compile(loss='mse', optimizer='sgd')\n",
        "callbacks = [keras.callbacks.EarlyStopping(\n",
        "    patience=5, min_delta = 1e-2)]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "customized_dense_layer_8 (Cu (None, 30)                270       \n",
            "_________________________________________________________________\n",
            "customized_dense_layer_9 (Cu (None, 1)                 31        \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 301\n",
            "Trainable params: 301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KonwzQFBMtMr",
        "colab_type": "code",
        "outputId": "d6858dab-bd77-4e86-d518-5c85195bd1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "history = model.fit(x_train_scaled, y_train,\n",
        "                    validation_data = (x_valid_scaled, y_valid),\n",
        "                    epochs = 100,\n",
        "                    callbacks = callbacks)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 97us/sample - loss: 1.1066 - val_loss: 0.6446\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 53us/sample - loss: 0.5588 - val_loss: 0.5529\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4923 - val_loss: 0.4990\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4654 - val_loss: 0.4869\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4501 - val_loss: 0.4592\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4371 - val_loss: 0.4559\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4263 - val_loss: 0.4426\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4193 - val_loss: 0.4287\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4118 - val_loss: 0.4238\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4138 - val_loss: 0.4306\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4064 - val_loss: 0.4142\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3981 - val_loss: 0.4091\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3929 - val_loss: 0.4166\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3902 - val_loss: 0.4018\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3862 - val_loss: 0.4063\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3851 - val_loss: 0.4135\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3852 - val_loss: 0.3949\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3816 - val_loss: 0.3920\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3798 - val_loss: 0.3902\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3781 - val_loss: 0.3915\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3757 - val_loss: 0.3962\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3800 - val_loss: 0.4062\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3743 - val_loss: 0.3859\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3728 - val_loss: 0.3854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwKk-GsPNIvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "84c8ad8c-7653-4c41-d409-f94e6526bc4c"
      },
      "source": [
        "def plot_learning_curves(history):\n",
        "  pd.DataFrame(history.history).plot(figsize = (8,5))\n",
        "  plt.grid(True)\n",
        "  plt.gca().set_ylim(0,1)\n",
        "  plt.show()\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF3UQ95bNkyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c6001935-9e93-4d79-f619-ad816d9424cf"
      },
      "source": [
        "model.evaluate(x_test_scaled, y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 31us/sample - loss: 0.3881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38812497124191403"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh9WL_ThxCBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir callbacks"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}